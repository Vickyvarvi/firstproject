{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWQGTzVTui3pfQRQVxeytd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vickyvarvi/firstproject/blob/main/Langchain_learning_chapter_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📘 LangChain Learning Chapters**\n",
        "\n",
        "**📖 Chapter 1: Introduction to LangChain**\n",
        "\n",
        "What is LangChain?\n",
        "\n",
        "Why use LangChain (vs direct API calls)?\n",
        "\n",
        "Core building blocks: Prompts, LLMs, Chains, Tools, Agents.\n",
        "\n",
        "👉 Mini Task: Print \"Hello LangChain!\" using Python.\n",
        "\n",
        "**📖 Chapter 2: Prompts**\n",
        "\n",
        "PromptTemplate – reusable prompts with variables.\n",
        "\n",
        "ChatPromptTemplate – multi-role prompts (system, human, ai).\n",
        "\n",
        "👉 Example:\n",
        "\n",
        "Create prompt template for resume builder.\n",
        "\n",
        "Translate English → Tamil.\n",
        "\n",
        "**📖 Chapter 3: LLMs**\n",
        "\n",
        "Connecting to LLMs:\n",
        "\n",
        "OpenAI (ChatOpenAI)\n",
        "\n",
        "Local (Ollama)\n",
        "\n",
        "HuggingFace models\n",
        "\n",
        ".invoke(), .predict(), .generate() methods.\n",
        "\n",
        "👉 Example: Simple chatbot using OpenAI.\n",
        "\n",
        "**📖 Chapter 4: Chains**\n",
        "\n",
        "LLMChain – combine prompt + LLM → output.\n",
        "\n",
        "SequentialChain – multiple steps in order.\n",
        "\n",
        "SimpleTransformChain – custom function chains.\n",
        "\n",
        "👉 Example:\n",
        "\n",
        "Step 1 → Generate title\n",
        "\n",
        "Step 2 → Generate blog intro\n",
        "\n",
        "Step 3 → Generate blog body\n",
        "\n",
        "**📖 Chapter 5: Memory**\n",
        "\n",
        "Why memory is needed (chatbots, assistants).\n",
        "\n",
        "ConversationBufferMemory – remembers last messages.\n",
        "\n",
        "ConversationSummaryMemory – summaries old chats.\n",
        "\n",
        "VectorStoreRetrieverMemory – long-term memory.\n",
        "\n",
        "👉 Example: Personal assistant bot that remembers your name.\n",
        "\n",
        "**📖 Chapter 6: Document Loading & Text Splitting**\n",
        "\n",
        "Document Loaders → PDF, CSV, Text, Web.\n",
        "\n",
        "TextSplitter → break large docs into chunks.\n",
        "\n",
        "👉 Example: Load a PDF and split it into 500-character chunks.\n",
        "\n",
        "**📖 Chapter 7: Embeddings & Vector Stores**\n",
        "\n",
        "Convert text → vectors.\n",
        "\n",
        "Embeddings models: OpenAI, HuggingFace.\n",
        "\n",
        "Vector DBs: FAISS, Chroma, pgvector.\n",
        "\n",
        "Similarity Search → Find top relevant docs.\n",
        "\n",
        "👉 Example: Store a PDF in FAISS and search answers.\n",
        "\n",
        "**📖 Chapter 8: Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "Combine retriever + LLM.\n",
        "\n",
        "User query → Retrieve → Feed into LLM → Answer.\n",
        "\n",
        "👉 Example: PDF Q&A bot.\n",
        "\n",
        "**📖 Chapter 9: Agents & Tools**\n",
        "\n",
        "AgentExecutor → LLM decides which tool to use.\n",
        "\n",
        "Built-in tools: Python REPL, SQL DB, API calls.\n",
        "\n",
        "Custom tools → your own Python functions.\n",
        "\n",
        "👉 Example:\n",
        "\n",
        "Agent that checks today’s weather using API.\n",
        "\n",
        "Agent that queries SQL database.\n",
        "\n",
        "**📖 Chapter 10: Output Parsers**\n",
        "\n",
        "Why parsing is needed (structured JSON, tables).\n",
        "\n",
        "SimpleOutputParser\n",
        "\n",
        "PydanticOutputParser (validate outputs).\n",
        "\n",
        "👉 Example: Parse LLM output into JSON (name, age, skills).\n",
        "\n",
        "**📖 Chapter 11: LangGraph (Advanced LangChain)**\n",
        "\n",
        "State-based workflow.\n",
        "\n",
        "Nodes, Edges, State.\n",
        "\n",
        "Looping & branching logic.\n",
        "\n",
        "👉 Example: Workflow – Load Doc → Extract Entities → Validate → Store.\n",
        "\n",
        "**📖 Chapter 12: Deployment**\n",
        "\n",
        "Run LangChain inside FastAPI or Django.\n",
        "\n",
        "Build simple frontend (Streamlit / React).\n",
        "\n",
        "Deploy to Vercel, AWS, or GCP.\n",
        "\n",
        "👉 Example: Deploy PDF Q&A bot with FastAPI."
      ],
      "metadata": {
        "id": "Zswou4ECU5kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts → PromptTemplate, ChatPromptTemplate"
      ],
      "metadata": {
        "id": "2tDO8ecIjLUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Prompt**"
      ],
      "metadata": {
        "id": "lZ2tdBRJjxKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OKIpJxhzjDuL"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Template define pannrathu\n",
        "template = \"Translate the following English text into Tamil:\\n\\n{text}\""
      ],
      "metadata": {
        "id": "JQajMQ-PjXfJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: PromptTemplate object create\n",
        "prompt = PromptTemplate(input_variables=[\"text\"], template=template)"
      ],
      "metadata": {
        "id": "-aPfQeRJjp2o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Fill values\n",
        "final_prompt = prompt.format(text=\"Good morning, how are you?\")\n",
        "\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSiePnphj4vf",
        "outputId": "6e5d7147-2837-4e5d-d3f9-0627438a96a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the following English text into Tamil:\n",
            "\n",
            "Good morning, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple** **Variables**"
      ],
      "metadata": {
        "id": "mHTyypO2kHOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Write a {tone} email to {recipient} about {topic}.\""
      ],
      "metadata": {
        "id": "WUnI20U0kM_3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"tone\", \"recipient\", \"topic\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "7hrjqgdCkScp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_prompt = prompt.format(\n",
        "    tone=\"formal\",\n",
        "    recipient=\"HR Manager\",\n",
        "    topic=\"leave application\"\n",
        ")\n",
        "\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6CmroESkgoP",
        "outputId": "364b7bb9-1930-43a0-b144-01a9295e0faa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a formal email to HR Manager about leave application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatPromptTemplate**"
      ],
      "metadata": {
        "id": "WY2RkOA7k6cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "fK6ySu0Xkz9R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Define multi-role prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful Tamil translator.\"),\n",
        "    (\"human\", \"Translate this text into Tamil: {text}\")\n",
        "])"
      ],
      "metadata": {
        "id": "znN0VGcBlZW4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Fill values\n",
        "final_prompt = prompt.format_messages(text=\"How are you?\")\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuoO4YAxlfy9",
        "outputId": "9e54661b-2d2d-44b6-e22d-a5661d6c1469"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a helpful Tamil translator.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate this text into Tamil: How are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 3: Large Language Models (LLMs)\n",
        "\n",
        "3.1 – What is an LLM in LangChain?\n",
        "LLM → A text-only model (like GPT-3.5, Gemini text-only, etc.)\n",
        "\n",
        "ChatModel → A conversational model (like Gemini, GPT-4 chat, etc.)\n",
        "\n",
        "👉 In LangChain, we usually use ChatModel (e.g., ChatGoogleGenerativeAI) since it supports role-based messages (system, human, ai).\n"
      ],
      "metadata": {
        "id": "Yvphu1PsmXvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-google-genai google-generativeai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OECZH2lwtfCR",
        "outputId": "fa6f9dfe-feb4-4ec7-ecdd-0db442601295"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.75 (from langchain-google-genai)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.74)\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.16)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.24.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatPromptTemplate + LLM**"
      ],
      "metadata": {
        "id": "oAgXxqq3wErF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeA"
      ],
      "metadata": {
        "id": "M2_eyO3ut52q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Set your Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GEMINI_API_KEY\""
      ],
      "metadata": {
        "id": "CHNLtDSFuEGf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a resume builder assistant.\"),\n",
        "    (\"human\", \"Generate a 2-line summary for a {role} with skills in {skills}.\")\n",
        "])"
      ],
      "metadata": {
        "id": "iVNXJ_0xuHmX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Fill placeholders\n",
        "final_prompt = prompt.format_messages(\n",
        "    role=\"Python Developer\",\n",
        "    skills=\"Django, LangChain\"\n",
        ")"
      ],
      "metadata": {
        "id": "hlDmdk20uMS2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Connect to Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "QzZxA77AuXR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Invoke the model with our prompt\n",
        "response = llm.invoke(final_prompt)"
      ],
      "metadata": {
        "id": "GG8XZezGubui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"👉 AI Resume Summary:\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "pnuTkZ7XuoKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Flow Recap:\n",
        "ChatPromptTemplate → reusable structured prompt (system + human).\n",
        "\n",
        "final_prompt → gets converted into SystemMessage + HumanMessage.\n",
        "\n",
        "llm.invoke(final_prompt) → sends it to Gemini.\n",
        "\n",
        "Gemini returns final text (resume summary)."
      ],
      "metadata": {
        "id": "YrOnt0nru76K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine** **PromptTemplate** + **LLM**"
      ],
      "metadata": {
        "id": "S3hbTZB8u_gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Prompt template\n",
        "template = \"Summarize the following text in 2 lines:\\n\\n{text}\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Format with input\n",
        "final_prompt = prompt.format(text=\"LangChain helps build LLM apps easily.\")\n",
        "\n",
        "# Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "response = llm.invoke(final_prompt)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "6St-wohHv8aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** What is invoke in LangChain?**\n",
        "\n",
        "In LangChain, LLM objects (like ChatGoogleGenerativeAI, ChatOpenAI) have methods to send input and get output.\n",
        "\n",
        "invoke() → Send one input and get one output (synchronous call).\n",
        "\n",
        "ainvoke() → Same as invoke but async (for async Python).\n",
        "\n",
        "batch() → Send a list of inputs and get a list of outputs.\n",
        "\n",
        "stream() → Get output token by token (like live streaming).\n",
        "\n"
      ],
      "metadata": {
        "id": "5nEnh-_twoPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📖 Chapter 3: LangChain — Chains**\n",
        "\n",
        "❓ What is a Chain?\n",
        "\n",
        "In real world, we rarely ask the LLM just one question.\n",
        "\n",
        "We want to combine multiple steps (e.g., prompt → LLM → parse → another LLM).\n",
        "\n",
        "A Chain connects these steps together.\n",
        "\n",
        "👉 Think of it like a pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "tbIkBOoJxqPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Simple LLMChain**\n",
        "#The most basic chain: Prompt → LLM → **Output**"
      ],
      "metadata": {
        "id": "6N0Myaf07MS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeA\n"
      ],
      "metadata": {
        "id": "FfSR93Bwxxwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "hjusOF6hx-LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"Give me a motivational quote about {topic}.\"\n",
        "prompt = PromptTemplate(input_variables=[\"topic\"], template=template)"
      ],
      "metadata": {
        "id": "AIXpEdTUyBUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n"
      ],
      "metadata": {
        "id": "tJAZhfowyJB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "response = chain.run(\"learning Python\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kja59_E8yOcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2️⃣ SequentialChain (Multiple Steps)**\n",
        "\n",
        "You can connect multiple prompts in sequence.\n",
        "\n",
        "Example:\n",
        "\n",
        "Step 1 → Generate a title\n",
        "\n",
        "Step 2 → Generate a summary based on the title"
      ],
      "metadata": {
        "id": "JQJNOzW5yiAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# Chain 1: Generate a title\n",
        "prompt1 = PromptTemplate.from_template(\"Give me a creative title about {topic}.\")\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1)\n"
      ],
      "metadata": {
        "id": "7XJO0DvtylKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 2: Generate a summary\n",
        "prompt2 = PromptTemplate.from_template(\"Write a 2-line summary for the title: {title}\")\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2)"
      ],
      "metadata": {
        "id": "47NgYMaByrdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential chain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain1, chain2])\n"
      ],
      "metadata": {
        "id": "ZuK9pb1ry3Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "print(overall_chain.run(\"Artificial Intelligence in Healthcare\"))"
      ],
      "metadata": {
        "id": "Ihr7d2AmzBx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3️⃣ SequentialChain (more powerful)**\n",
        "\n",
        "Unlike SimpleSequentialChain, here we can pass multiple variables across steps."
      ],
      "metadata": {
        "id": "3X3OXs3S7e51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# Chain 1: Generate a title\n",
        "prompt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Give me a creative title about {topic}.\")\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"title\")\n"
      ],
      "metadata": {
        "id": "UDTHRoAc7iMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 2: Generate summary using title + topic\n",
        "prompt2 = PromptTemplate(\n",
        "    input_variables=[\"title\", \"topic\"],\n",
        "    template=\"Write a 2-line summary for the blog '{title}' on topic {topic}.\"\n",
        ")\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"summary\")\n"
      ],
      "metadata": {
        "id": "KC2asShQ7mJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SequentialChain with multiple inputs/outputs\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain1, chain2],\n",
        "    input_variables=[\"topic\"],\n",
        "    output_variables=[\"title\", \"summary\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "9_ENAHMR7pER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = overall_chain({\"topic\": \"AI in Finance\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qlcj0bmS7uU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference from SimpleSequentialChain:\n",
        "\n",
        "Supports dictionaries (multiple inputs/outputs)\n",
        "\n",
        "More flexible for complex workflows"
      ],
      "metadata": {
        "id": "S3hepCMn7zYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4️⃣ RouterChain**\n",
        "\n",
        "What if you want to route input to different prompts?\n",
        "E.g., if the topic is \"science\", use science prompt, if \"history\", use history prompt.\n",
        "\n",
        "This is like an intelligent switchboard."
      ],
      "metadata": {
        "id": "OQSsysID73SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "\n",
        "# Different prompt templates\n",
        "science_prompt = PromptTemplate.from_template(\"Explain {question} like a science teacher.\")\n",
        "history_prompt = PromptTemplate.from_template(\"Explain {question} like a history professor.\")\n"
      ],
      "metadata": {
        "id": "7p7zc6Jd78FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define destinations\n",
        "destination_chains = {\n",
        "    \"science\": LLMChain(llm=llm, prompt=science_prompt),\n",
        "    \"history\": LLMChain(llm=llm, prompt=history_prompt),\n",
        "}"
      ],
      "metadata": {
        "id": "GQBq9DOT8EbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default fallback\n",
        "default_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Explain {question} briefly.\"))\n"
      ],
      "metadata": {
        "id": "S80nO9mv8Ir2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Router Chain\n",
        "router_chain = MultiPromptChain(\n",
        "    llm=llm,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain\n",
        ")\n"
      ],
      "metadata": {
        "id": "fKoGIr0W8LOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(router_chain.run(\"What is gravity?\"))\n",
        "print(router_chain.run(\"Who was Napoleon?\"))\n",
        "#✅ Output will automatically go to the correct chain depending on input."
      ],
      "metadata": {
        "id": "-IIhh8j-8OAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 **Memory** **in** **LangChain**\n",
        "1. What is Memory?\n",
        "Normally, when you talk to an LLM (like Gemini, GPT), it does not remember past conversations.\n"
      ],
      "metadata": {
        "id": "LP6ROsx5KUVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Types of Memory in LangChain**\n",
        "\n",
        "LangChain gives different memory styles:\n",
        "\n",
        "**ConversationBufferMemory**\n",
        "\n",
        "Remembers the full chat history.\n",
        "\n",
        "Example: “ChatGPT style memory.”\n",
        "\n",
        "**ConversationBufferWindowMemory**\n",
        "\n",
        "Remembers only the last N messages (like a sliding window).\n",
        "\n",
        "Useful when you don’t want to overload the context.\n",
        "\n",
        "**ConversationSummaryMemory**\n",
        "\n",
        "Summarizes old chats, keeps only key points.\n",
        "\n",
        "Helps when context is long but you still need memory.\n",
        "\n",
        "**EntityMemory**\n",
        "\n",
        "Remembers specific facts about people, places, things.\n",
        "\n",
        "Example: If you say “My dog is Max,” later it remembers Max is your dog."
      ],
      "metadata": {
        "id": "635dLbzvLMah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Conversation Memory**"
      ],
      "metadata": {
        "id": "uvRIO3FmMhzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# Memory object\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Conversation chain with memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "conversation.run(\"Hello, my name is Vignesh.\")\n",
        "conversation.run(\"I live in Chennai.\")\n",
        "conversation.run(\"Do you remember my name?\")\n"
      ],
      "metadata": {
        "id": "66mHyJLULhxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 1 – ConversationBufferMemory (Full History)**\n"
      ],
      "metadata": {
        "id": "JTUODA9XMwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# Memory (stores full conversation)\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Chain with memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"Hello, I am Vignesh.\"))\n",
        "print(conversation.run(\"I live in Chennai.\"))\n",
        "print(conversation.run(\"What is my name?\"))\n",
        "print(conversation.run(\"Where do I live?\"))\n"
      ],
      "metadata": {
        "id": "5BtjIfOoMztS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 2 – ConversationBufferWindowMemory (Sliding Window)**"
      ],
      "metadata": {
        "id": "Auw7WC1VM30i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2)  # remembers only last 2 exchanges\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"My favorite color is blue.\"))\n",
        "print(conversation.run(\"I like cricket.\"))\n",
        "print(conversation.run(\"I work as a data analyst.\"))\n",
        "print(conversation.run(\"What is my favorite color?\"))\n"
      ],
      "metadata": {
        "id": "a4wGAkCONDxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 3 – ConversationSummaryMemory (Summarized)**"
      ],
      "metadata": {
        "id": "RQaYoY81NNMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"I am Vignesh. I work in supply chain automation.\"))\n",
        "print(conversation.run(\"I am building an AI RFQ tool.\"))\n",
        "print(conversation.run(\"Can you remind me what I am working on?\"))\n"
      ],
      "metadata": {
        "id": "Phop_FDGNPka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 4 – EntityMemory (Facts about people/objects)**"
      ],
      "metadata": {
        "id": "wZ9SC1t0NZFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationEntityMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationEntityMemory(llm=llm)\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"My dog’s name is Max.\"))\n",
        "print(conversation.run(\"Max loves playing fetch.\"))\n",
        "print(conversation.run(\"What is my dog’s name?\"))\n"
      ],
      "metadata": {
        "id": "znezcU2qNeY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 5 – Choosing the Right Memory**\n",
        "\n",
        "Use BufferMemory if you need full history (short convos).\n",
        "\n",
        "Use WindowMemory if you want only recent history.\n",
        "\n",
        "Use SummaryMemory for long chats.\n",
        "\n",
        "Use EntityMemory for structured facts"
      ],
      "metadata": {
        "id": "Ya_29cetNqYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📘 Chapter 6: Document Loading (LangChain)**\n",
        "\n",
        "When building RAG or any knowledge-based chatbot, you need to load documents first (PDF, text, CSV, web pages, etc.) before embedding and querying.\n",
        "\n",
        "LangChain gives you Document Loaders for this.\n",
        "\n",
        "**6.1 🔹 What is a Document?**\n",
        "\n",
        "In LangChain, a Document is a Python object that has:\n",
        "\n",
        "page_content → the text inside\n",
        "\n",
        "metadata → info like filename, page number, source, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "XGa1pa9-FyRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#📂 Loading a Text File\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"example.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(documents[0].page_content[:200])  # first 200 chars\n",
        "print(documents[0].metadata)"
      ],
      "metadata": {
        "id": "s4Wh9EcsF6XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#📄 Loading a PDF\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"example.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(len(documents))  # number of pages\n",
        "print(documents[0].page_content[:200])\n"
      ],
      "metadata": {
        "id": "12kixlGhGHMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#🌍 Loading from a Website\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.langchain.com\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(documents[0].page_content[:200])\n"
      ],
      "metadata": {
        "id": "23EFSHZ1GeXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.3 🔹 Splitting Large Documents**\n",
        "\n",
        "If a document is very big, you split it into chunks before embedding.\n",
        "\n"
      ],
      "metadata": {
        "id": "Htr5hpp3GrJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(len(docs))  # number of chunks\n",
        "print(docs[0].page_content)\n"
      ],
      "metadata": {
        "id": "9HJCqTZxGw9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.4 🔹 Typical Flow**\n",
        "\n",
        "Load documents (PDF, CSV, website, etc.)\n",
        "\n",
        "Split into chunks\n",
        "\n",
        "Embed into vector DB (next chapter)\n",
        "\n",
        "Use RAG pipeline to answer questions\n",
        "\n"
      ],
      "metadata": {
        "id": "SnL294JLG6OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 7 – Embeddings + Vector Stores**\n",
        "\n",
        "This is the core of RAG (Retrieval-Augmented Generation).\n",
        "We take the chunks from the PDF and convert them into vector embeddings (numerical representation of text). Then, we store them in a Vector DB (like Chroma, FAISS, Pinecone)."
      ],
      "metadata": {
        "id": "ehz4emXnG_4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 **Step 1: Install requirements**"
      ],
      "metadata": {
        "id": "DZGmQ_oOIqws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-chroma sentence-transformers\n"
      ],
      "metadata": {
        "id": "HAQHqki2Ixeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 **Step 2: Create embeddings**"
      ],
      "metadata": {
        "id": "Ql7RanEtIzax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Create embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Example text\n",
        "text = \"LangChain is a framework for developing applications powered by LLMs.\"\n",
        "\n",
        "vector = embeddings.embed_query(text)\n",
        "print(\"🔹 Vector length:\", len(vector))\n",
        "print(\"🔹 First 5 numbers:\", vector[:5])\n",
        "#👉 You’ll see a list of floating numbers (vector). That’s how the model represents meaning."
      ],
      "metadata": {
        "id": "rJtegidFI2sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 **Step 3: Store in VectorDB (Chroma)**"
      ],
      "metadata": {
        "id": "Zk8q4EMrJAX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Assume `docs` is from Chapter 6 (PDF chunks)\n",
        "db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n",
        "\n",
        "print(\"✅ Vector DB created with\", db._collection.count(), \"documents\")"
      ],
      "metadata": {
        "id": "DcltLN2dJN_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 **Step 4: Search from VectorDB**"
      ],
      "metadata": {
        "id": "DXCW_ORdJVfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "query = \"What is LangChain?\"\n",
        "\n",
        "# Get top 2 similar chunks\n",
        "results = db.similarity_search(query, k=2)\n",
        "\n",
        "for i, res in enumerate(results):\n",
        "    print(f\"\\n🔹 Result {i+1}:\")\n",
        "    print(res.page_content)\n",
        "    print(\"Metadata:\", res.metadata)\n"
      ],
      "metadata": {
        "id": "_4COPXYFJYl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}