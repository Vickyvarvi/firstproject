{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWQGTzVTui3pfQRQVxeytd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vickyvarvi/firstproject/blob/main/Langchain_learning_chapter_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìò LangChain Learning Chapters**\n",
        "\n",
        "**üìñ Chapter 1: Introduction to LangChain**\n",
        "\n",
        "What is LangChain?\n",
        "\n",
        "Why use LangChain (vs direct API calls)?\n",
        "\n",
        "Core building blocks: Prompts, LLMs, Chains, Tools, Agents.\n",
        "\n",
        "üëâ Mini Task: Print \"Hello LangChain!\" using Python.\n",
        "\n",
        "**üìñ Chapter 2: Prompts**\n",
        "\n",
        "PromptTemplate ‚Äì reusable prompts with variables.\n",
        "\n",
        "ChatPromptTemplate ‚Äì multi-role prompts (system, human, ai).\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "Create prompt template for resume builder.\n",
        "\n",
        "Translate English ‚Üí Tamil.\n",
        "\n",
        "**üìñ Chapter 3: LLMs**\n",
        "\n",
        "Connecting to LLMs:\n",
        "\n",
        "OpenAI (ChatOpenAI)\n",
        "\n",
        "Local (Ollama)\n",
        "\n",
        "HuggingFace models\n",
        "\n",
        ".invoke(), .predict(), .generate() methods.\n",
        "\n",
        "üëâ Example: Simple chatbot using OpenAI.\n",
        "\n",
        "**üìñ Chapter 4: Chains**\n",
        "\n",
        "LLMChain ‚Äì combine prompt + LLM ‚Üí output.\n",
        "\n",
        "SequentialChain ‚Äì multiple steps in order.\n",
        "\n",
        "SimpleTransformChain ‚Äì custom function chains.\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "Step 1 ‚Üí Generate title\n",
        "\n",
        "Step 2 ‚Üí Generate blog intro\n",
        "\n",
        "Step 3 ‚Üí Generate blog body\n",
        "\n",
        "**üìñ Chapter 5: Memory**\n",
        "\n",
        "Why memory is needed (chatbots, assistants).\n",
        "\n",
        "ConversationBufferMemory ‚Äì remembers last messages.\n",
        "\n",
        "ConversationSummaryMemory ‚Äì summaries old chats.\n",
        "\n",
        "VectorStoreRetrieverMemory ‚Äì long-term memory.\n",
        "\n",
        "üëâ Example: Personal assistant bot that remembers your name.\n",
        "\n",
        "**üìñ Chapter 6: Document Loading & Text Splitting**\n",
        "\n",
        "Document Loaders ‚Üí PDF, CSV, Text, Web.\n",
        "\n",
        "TextSplitter ‚Üí break large docs into chunks.\n",
        "\n",
        "üëâ Example: Load a PDF and split it into 500-character chunks.\n",
        "\n",
        "**üìñ Chapter 7: Embeddings & Vector Stores**\n",
        "\n",
        "Convert text ‚Üí vectors.\n",
        "\n",
        "Embeddings models: OpenAI, HuggingFace.\n",
        "\n",
        "Vector DBs: FAISS, Chroma, pgvector.\n",
        "\n",
        "Similarity Search ‚Üí Find top relevant docs.\n",
        "\n",
        "üëâ Example: Store a PDF in FAISS and search answers.\n",
        "\n",
        "**üìñ Chapter 8: Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "Combine retriever + LLM.\n",
        "\n",
        "User query ‚Üí Retrieve ‚Üí Feed into LLM ‚Üí Answer.\n",
        "\n",
        "üëâ Example: PDF Q&A bot.\n",
        "\n",
        "**üìñ Chapter 9: Agents & Tools**\n",
        "\n",
        "AgentExecutor ‚Üí LLM decides which tool to use.\n",
        "\n",
        "Built-in tools: Python REPL, SQL DB, API calls.\n",
        "\n",
        "Custom tools ‚Üí your own Python functions.\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "Agent that checks today‚Äôs weather using API.\n",
        "\n",
        "Agent that queries SQL database.\n",
        "\n",
        "**üìñ Chapter 10: Output Parsers**\n",
        "\n",
        "Why parsing is needed (structured JSON, tables).\n",
        "\n",
        "SimpleOutputParser\n",
        "\n",
        "PydanticOutputParser (validate outputs).\n",
        "\n",
        "üëâ Example: Parse LLM output into JSON (name, age, skills).\n",
        "\n",
        "**üìñ Chapter 11: LangGraph (Advanced LangChain)**\n",
        "\n",
        "State-based workflow.\n",
        "\n",
        "Nodes, Edges, State.\n",
        "\n",
        "Looping & branching logic.\n",
        "\n",
        "üëâ Example: Workflow ‚Äì Load Doc ‚Üí Extract Entities ‚Üí Validate ‚Üí Store.\n",
        "\n",
        "**üìñ Chapter 12: Deployment**\n",
        "\n",
        "Run LangChain inside FastAPI or Django.\n",
        "\n",
        "Build simple frontend (Streamlit / React).\n",
        "\n",
        "Deploy to Vercel, AWS, or GCP.\n",
        "\n",
        "üëâ Example: Deploy PDF Q&A bot with FastAPI."
      ],
      "metadata": {
        "id": "Zswou4ECU5kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts ‚Üí PromptTemplate, ChatPromptTemplate"
      ],
      "metadata": {
        "id": "2tDO8ecIjLUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Prompt**"
      ],
      "metadata": {
        "id": "lZ2tdBRJjxKO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OKIpJxhzjDuL"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Template define pannrathu\n",
        "template = \"Translate the following English text into Tamil:\\n\\n{text}\""
      ],
      "metadata": {
        "id": "JQajMQ-PjXfJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: PromptTemplate object create\n",
        "prompt = PromptTemplate(input_variables=[\"text\"], template=template)"
      ],
      "metadata": {
        "id": "-aPfQeRJjp2o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Fill values\n",
        "final_prompt = prompt.format(text=\"Good morning, how are you?\")\n",
        "\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSiePnphj4vf",
        "outputId": "6e5d7147-2837-4e5d-d3f9-0627438a96a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the following English text into Tamil:\n",
            "\n",
            "Good morning, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple** **Variables**"
      ],
      "metadata": {
        "id": "mHTyypO2kHOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Write a {tone} email to {recipient} about {topic}.\""
      ],
      "metadata": {
        "id": "WUnI20U0kM_3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"tone\", \"recipient\", \"topic\"],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "7hrjqgdCkScp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_prompt = prompt.format(\n",
        "    tone=\"formal\",\n",
        "    recipient=\"HR Manager\",\n",
        "    topic=\"leave application\"\n",
        ")\n",
        "\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6CmroESkgoP",
        "outputId": "364b7bb9-1930-43a0-b144-01a9295e0faa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a formal email to HR Manager about leave application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatPromptTemplate**"
      ],
      "metadata": {
        "id": "WY2RkOA7k6cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "fK6ySu0Xkz9R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Define multi-role prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful Tamil translator.\"),\n",
        "    (\"human\", \"Translate this text into Tamil: {text}\")\n",
        "])"
      ],
      "metadata": {
        "id": "znN0VGcBlZW4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Fill values\n",
        "final_prompt = prompt.format_messages(text=\"How are you?\")\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuoO4YAxlfy9",
        "outputId": "9e54661b-2d2d-44b6-e22d-a5661d6c1469"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a helpful Tamil translator.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate this text into Tamil: How are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 3: Large Language Models (LLMs)\n",
        "\n",
        "3.1 ‚Äì What is an LLM in LangChain?\n",
        "LLM ‚Üí A text-only model (like GPT-3.5, Gemini text-only, etc.)\n",
        "\n",
        "ChatModel ‚Üí A conversational model (like Gemini, GPT-4 chat, etc.)\n",
        "\n",
        "üëâ In LangChain, we usually use ChatModel (e.g., ChatGoogleGenerativeAI) since it supports role-based messages (system, human, ai).\n"
      ],
      "metadata": {
        "id": "Yvphu1PsmXvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-google-genai google-generativeai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OECZH2lwtfCR",
        "outputId": "fa6f9dfe-feb4-4ec7-ecdd-0db442601295"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.75 (from langchain-google-genai)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.74)\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.16)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.24.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatPromptTemplate + LLM**"
      ],
      "metadata": {
        "id": "oAgXxqq3wErF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeA"
      ],
      "metadata": {
        "id": "M2_eyO3ut52q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üëâ Set your Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GEMINI_API_KEY\""
      ],
      "metadata": {
        "id": "CHNLtDSFuEGf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a resume builder assistant.\"),\n",
        "    (\"human\", \"Generate a 2-line summary for a {role} with skills in {skills}.\")\n",
        "])"
      ],
      "metadata": {
        "id": "iVNXJ_0xuHmX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Fill placeholders\n",
        "final_prompt = prompt.format_messages(\n",
        "    role=\"Python Developer\",\n",
        "    skills=\"Django, LangChain\"\n",
        ")"
      ],
      "metadata": {
        "id": "hlDmdk20uMS2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Connect to Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "QzZxA77AuXR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Invoke the model with our prompt\n",
        "response = llm.invoke(final_prompt)"
      ],
      "metadata": {
        "id": "GG8XZezGubui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üëâ AI Resume Summary:\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "pnuTkZ7XuoKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Flow Recap:\n",
        "ChatPromptTemplate ‚Üí reusable structured prompt (system + human).\n",
        "\n",
        "final_prompt ‚Üí gets converted into SystemMessage + HumanMessage.\n",
        "\n",
        "llm.invoke(final_prompt) ‚Üí sends it to Gemini.\n",
        "\n",
        "Gemini returns final text (resume summary)."
      ],
      "metadata": {
        "id": "YrOnt0nru76K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine** **PromptTemplate** + **LLM**"
      ],
      "metadata": {
        "id": "S3hbTZB8u_gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Prompt template\n",
        "template = \"Summarize the following text in 2 lines:\\n\\n{text}\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Format with input\n",
        "final_prompt = prompt.format(text=\"LangChain helps build LLM apps easily.\")\n",
        "\n",
        "# Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "response = llm.invoke(final_prompt)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "6St-wohHv8aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** What is invoke in LangChain?**\n",
        "\n",
        "In LangChain, LLM objects (like ChatGoogleGenerativeAI, ChatOpenAI) have methods to send input and get output.\n",
        "\n",
        "invoke() ‚Üí Send one input and get one output (synchronous call).\n",
        "\n",
        "ainvoke() ‚Üí Same as invoke but async (for async Python).\n",
        "\n",
        "batch() ‚Üí Send a list of inputs and get a list of outputs.\n",
        "\n",
        "stream() ‚Üí Get output token by token (like live streaming).\n",
        "\n"
      ],
      "metadata": {
        "id": "5nEnh-_twoPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìñ Chapter 3: LangChain ‚Äî Chains**\n",
        "\n",
        "‚ùì What is a Chain?\n",
        "\n",
        "In real world, we rarely ask the LLM just one question.\n",
        "\n",
        "We want to combine multiple steps (e.g., prompt ‚Üí LLM ‚Üí parse ‚Üí another LLM).\n",
        "\n",
        "A Chain connects these steps together.\n",
        "\n",
        "üëâ Think of it like a pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "tbIkBOoJxqPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Simple LLMChain**\n",
        "#The most basic chain: Prompt ‚Üí LLM ‚Üí **Output**"
      ],
      "metadata": {
        "id": "6N0Myaf07MS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeA\n"
      ],
      "metadata": {
        "id": "FfSR93Bwxxwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "hjusOF6hx-LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"Give me a motivational quote about {topic}.\"\n",
        "prompt = PromptTemplate(input_variables=[\"topic\"], template=template)"
      ],
      "metadata": {
        "id": "AIXpEdTUyBUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n"
      ],
      "metadata": {
        "id": "tJAZhfowyJB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "response = chain.run(\"learning Python\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kja59_E8yOcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2Ô∏è‚É£ SequentialChain (Multiple Steps)**\n",
        "\n",
        "You can connect multiple prompts in sequence.\n",
        "\n",
        "Example:\n",
        "\n",
        "Step 1 ‚Üí Generate a title\n",
        "\n",
        "Step 2 ‚Üí Generate a summary based on the title"
      ],
      "metadata": {
        "id": "JQJNOzW5yiAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# Chain 1: Generate a title\n",
        "prompt1 = PromptTemplate.from_template(\"Give me a creative title about {topic}.\")\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1)\n"
      ],
      "metadata": {
        "id": "7XJO0DvtylKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 2: Generate a summary\n",
        "prompt2 = PromptTemplate.from_template(\"Write a 2-line summary for the title: {title}\")\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2)"
      ],
      "metadata": {
        "id": "47NgYMaByrdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential chain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain1, chain2])\n"
      ],
      "metadata": {
        "id": "ZuK9pb1ry3Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "print(overall_chain.run(\"Artificial Intelligence in Healthcare\"))"
      ],
      "metadata": {
        "id": "Ihr7d2AmzBx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3Ô∏è‚É£ SequentialChain (more powerful)**\n",
        "\n",
        "Unlike SimpleSequentialChain, here we can pass multiple variables across steps."
      ],
      "metadata": {
        "id": "3X3OXs3S7e51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# Chain 1: Generate a title\n",
        "prompt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Give me a creative title about {topic}.\")\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"title\")\n"
      ],
      "metadata": {
        "id": "UDTHRoAc7iMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain 2: Generate summary using title + topic\n",
        "prompt2 = PromptTemplate(\n",
        "    input_variables=[\"title\", \"topic\"],\n",
        "    template=\"Write a 2-line summary for the blog '{title}' on topic {topic}.\"\n",
        ")\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"summary\")\n"
      ],
      "metadata": {
        "id": "KC2asShQ7mJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SequentialChain with multiple inputs/outputs\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain1, chain2],\n",
        "    input_variables=[\"topic\"],\n",
        "    output_variables=[\"title\", \"summary\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "9_ENAHMR7pER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = overall_chain({\"topic\": \"AI in Finance\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qlcj0bmS7uU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference from SimpleSequentialChain:\n",
        "\n",
        "Supports dictionaries (multiple inputs/outputs)\n",
        "\n",
        "More flexible for complex workflows"
      ],
      "metadata": {
        "id": "S3hepCMn7zYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4Ô∏è‚É£ RouterChain**\n",
        "\n",
        "What if you want to route input to different prompts?\n",
        "E.g., if the topic is \"science\", use science prompt, if \"history\", use history prompt.\n",
        "\n",
        "This is like an intelligent switchboard."
      ],
      "metadata": {
        "id": "OQSsysID73SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "\n",
        "# Different prompt templates\n",
        "science_prompt = PromptTemplate.from_template(\"Explain {question} like a science teacher.\")\n",
        "history_prompt = PromptTemplate.from_template(\"Explain {question} like a history professor.\")\n"
      ],
      "metadata": {
        "id": "7p7zc6Jd78FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define destinations\n",
        "destination_chains = {\n",
        "    \"science\": LLMChain(llm=llm, prompt=science_prompt),\n",
        "    \"history\": LLMChain(llm=llm, prompt=history_prompt),\n",
        "}"
      ],
      "metadata": {
        "id": "GQBq9DOT8EbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default fallback\n",
        "default_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(\"Explain {question} briefly.\"))\n"
      ],
      "metadata": {
        "id": "S80nO9mv8Ir2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Router Chain\n",
        "router_chain = MultiPromptChain(\n",
        "    llm=llm,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain\n",
        ")\n"
      ],
      "metadata": {
        "id": "fKoGIr0W8LOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(router_chain.run(\"What is gravity?\"))\n",
        "print(router_chain.run(\"Who was Napoleon?\"))\n",
        "#‚úÖ Output will automatically go to the correct chain depending on input."
      ],
      "metadata": {
        "id": "-IIhh8j-8OAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† **Memory** **in** **LangChain**\n",
        "1. What is Memory?\n",
        "Normally, when you talk to an LLM (like Gemini, GPT), it does not remember past conversations.\n"
      ],
      "metadata": {
        "id": "LP6ROsx5KUVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Types of Memory in LangChain**\n",
        "\n",
        "LangChain gives different memory styles:\n",
        "\n",
        "**ConversationBufferMemory**\n",
        "\n",
        "Remembers the full chat history.\n",
        "\n",
        "Example: ‚ÄúChatGPT style memory.‚Äù\n",
        "\n",
        "**ConversationBufferWindowMemory**\n",
        "\n",
        "Remembers only the last N messages (like a sliding window).\n",
        "\n",
        "Useful when you don‚Äôt want to overload the context.\n",
        "\n",
        "**ConversationSummaryMemory**\n",
        "\n",
        "Summarizes old chats, keeps only key points.\n",
        "\n",
        "Helps when context is long but you still need memory.\n",
        "\n",
        "**EntityMemory**\n",
        "\n",
        "Remembers specific facts about people, places, things.\n",
        "\n",
        "Example: If you say ‚ÄúMy dog is Max,‚Äù later it remembers Max is your dog."
      ],
      "metadata": {
        "id": "635dLbzvLMah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Conversation Memory**"
      ],
      "metadata": {
        "id": "uvRIO3FmMhzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# Memory object\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Conversation chain with memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "conversation.run(\"Hello, my name is Vignesh.\")\n",
        "conversation.run(\"I live in Chennai.\")\n",
        "conversation.run(\"Do you remember my name?\")\n"
      ],
      "metadata": {
        "id": "66mHyJLULhxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 1 ‚Äì ConversationBufferMemory (Full History)**\n"
      ],
      "metadata": {
        "id": "JTUODA9XMwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "# Memory (stores full conversation)\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Chain with memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"Hello, I am Vignesh.\"))\n",
        "print(conversation.run(\"I live in Chennai.\"))\n",
        "print(conversation.run(\"What is my name?\"))\n",
        "print(conversation.run(\"Where do I live?\"))\n"
      ],
      "metadata": {
        "id": "5BtjIfOoMztS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 2 ‚Äì ConversationBufferWindowMemory (Sliding Window)**"
      ],
      "metadata": {
        "id": "Auw7WC1VM30i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2)  # remembers only last 2 exchanges\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"My favorite color is blue.\"))\n",
        "print(conversation.run(\"I like cricket.\"))\n",
        "print(conversation.run(\"I work as a data analyst.\"))\n",
        "print(conversation.run(\"What is my favorite color?\"))\n"
      ],
      "metadata": {
        "id": "a4wGAkCONDxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 3 ‚Äì ConversationSummaryMemory (Summarized)**"
      ],
      "metadata": {
        "id": "RQaYoY81NNMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"I am Vignesh. I work in supply chain automation.\"))\n",
        "print(conversation.run(\"I am building an AI RFQ tool.\"))\n",
        "print(conversation.run(\"Can you remind me what I am working on?\"))\n"
      ],
      "metadata": {
        "id": "Phop_FDGNPka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 4 ‚Äì EntityMemory (Facts about people/objects)**"
      ],
      "metadata": {
        "id": "wZ9SC1t0NZFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationEntityMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationEntityMemory(llm=llm)\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "print(conversation.run(\"My dog‚Äôs name is Max.\"))\n",
        "print(conversation.run(\"Max loves playing fetch.\"))\n",
        "print(conversation.run(\"What is my dog‚Äôs name?\"))\n"
      ],
      "metadata": {
        "id": "znezcU2qNeY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 5 ‚Äì Choosing the Right Memory**\n",
        "\n",
        "Use BufferMemory if you need full history (short convos).\n",
        "\n",
        "Use WindowMemory if you want only recent history.\n",
        "\n",
        "Use SummaryMemory for long chats.\n",
        "\n",
        "Use EntityMemory for structured facts"
      ],
      "metadata": {
        "id": "Ya_29cetNqYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìò Chapter 6: Document Loading (LangChain)**\n",
        "\n",
        "When building RAG or any knowledge-based chatbot, you need to load documents first (PDF, text, CSV, web pages, etc.) before embedding and querying.\n",
        "\n",
        "LangChain gives you Document Loaders for this.\n",
        "\n",
        "**6.1 üîπ What is a Document?**\n",
        "\n",
        "In LangChain, a Document is a Python object that has:\n",
        "\n",
        "page_content ‚Üí the text inside\n",
        "\n",
        "metadata ‚Üí info like filename, page number, source, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "XGa1pa9-FyRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#üìÇ Loading a Text File\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"example.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(documents[0].page_content[:200])  # first 200 chars\n",
        "print(documents[0].metadata)"
      ],
      "metadata": {
        "id": "s4Wh9EcsF6XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#üìÑ Loading a PDF\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"example.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(len(documents))  # number of pages\n",
        "print(documents[0].page_content[:200])\n"
      ],
      "metadata": {
        "id": "12kixlGhGHMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#üåç Loading from a Website\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.langchain.com\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(documents[0].page_content[:200])\n"
      ],
      "metadata": {
        "id": "23EFSHZ1GeXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.3 üîπ Splitting Large Documents**\n",
        "\n",
        "If a document is very big, you split it into chunks before embedding.\n",
        "\n"
      ],
      "metadata": {
        "id": "Htr5hpp3GrJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(len(docs))  # number of chunks\n",
        "print(docs[0].page_content)\n"
      ],
      "metadata": {
        "id": "9HJCqTZxGw9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.4 üîπ Typical Flow**\n",
        "\n",
        "Load documents (PDF, CSV, website, etc.)\n",
        "\n",
        "Split into chunks\n",
        "\n",
        "Embed into vector DB (next chapter)\n",
        "\n",
        "Use RAG pipeline to answer questions\n",
        "\n"
      ],
      "metadata": {
        "id": "SnL294JLG6OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter 7 ‚Äì Embeddings + Vector Stores**\n",
        "\n",
        "This is the core of RAG (Retrieval-Augmented Generation).\n",
        "We take the chunks from the PDF and convert them into vector embeddings (numerical representation of text). Then, we store them in a Vector DB (like Chroma, FAISS, Pinecone)."
      ],
      "metadata": {
        "id": "ehz4emXnG_4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© **Step 1: Install requirements**"
      ],
      "metadata": {
        "id": "DZGmQ_oOIqws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-chroma sentence-transformers\n"
      ],
      "metadata": {
        "id": "HAQHqki2Ixeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© **Step 2: Create embeddings**"
      ],
      "metadata": {
        "id": "Ql7RanEtIzax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Create embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Example text\n",
        "text = \"LangChain is a framework for developing applications powered by LLMs.\"\n",
        "\n",
        "vector = embeddings.embed_query(text)\n",
        "print(\"üîπ Vector length:\", len(vector))\n",
        "print(\"üîπ First 5 numbers:\", vector[:5])\n",
        "#üëâ You‚Äôll see a list of floating numbers (vector). That‚Äôs how the model represents meaning."
      ],
      "metadata": {
        "id": "rJtegidFI2sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© **Step 3: Store in VectorDB (Chroma)**"
      ],
      "metadata": {
        "id": "Zk8q4EMrJAX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Assume `docs` is from Chapter 6 (PDF chunks)\n",
        "db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n",
        "\n",
        "print(\"‚úÖ Vector DB created with\", db._collection.count(), \"documents\")"
      ],
      "metadata": {
        "id": "DcltLN2dJN_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© **Step 4: Search from VectorDB**"
      ],
      "metadata": {
        "id": "DXCW_ORdJVfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "query = \"What is LangChain?\"\n",
        "\n",
        "# Get top 2 similar chunks\n",
        "results = db.similarity_search(query, k=2)\n",
        "\n",
        "for i, res in enumerate(results):\n",
        "    print(f\"\\nüîπ Result {i+1}:\")\n",
        "    print(res.page_content)\n",
        "    print(\"Metadata:\", res.metadata)\n"
      ],
      "metadata": {
        "id": "_4COPXYFJYl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}